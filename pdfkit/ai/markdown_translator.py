"""AI Markdown ç¿»è¯‘å™¨ - VLè¯†åˆ« + MTç¿»è¯‘çš„ä¸¤é˜¶æ®µæ¶æ„

ä½¿ç”¨ Qwen3-VL è§†è§‰æ¨¡å‹æå–åŸæ–‡ç»“æ„ï¼Œå†ä½¿ç”¨ qwen-mt-plus ä¸“ç”¨ç¿»è¯‘æ¨¡å‹è¿›è¡Œç¿»è¯‘ã€‚
"""

import csv
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Tuple
from PIL import Image
import fitz  # PyMuPDF

from .qwen_mt_translator import QwenMTTranslator
from ..core.ocr_handler import QwenVLOCR


# VLæ¨¡å‹è¯†åˆ«æç¤ºè¯ - ä¿ç•™Markdownç»“æ„
OCR_EXTRACT_PROMPT = """è¯·è¯†åˆ«å¹¶æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ã€‚

è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„
2. æ ‡é¢˜ä½¿ç”¨ # ## ### ç­‰ Markdown æ ¼å¼
3. åˆ—è¡¨ä¿æŒ - æˆ– 1. 2. æ ¼å¼
4. è¡¨æ ¼ä½¿ç”¨ Markdown è¡¨æ ¼æ ¼å¼
5. ä»£ç å—ä½¿ç”¨ ``` åŒ…è£¹
6. å…¬å¼ä½¿ç”¨ $...$ æˆ– $$...$$ æ ¼å¼
7. åªè¾“å‡ºåŸæ–‡ï¼Œä¸è¦ç¿»è¯‘ï¼Œä¸è¦æ·»åŠ è§£é‡Š

ç›´æ¥è¾“å‡ºè¯†åˆ«åˆ°çš„æ–‡å­—å†…å®¹ã€‚
"""


def pdf_page_to_image(page: fitz.Page, dpi: int = 300) -> Image.Image:
    """å°† PDF é¡µé¢è½¬æ¢ä¸º PIL Image

    Args:
        page: PyMuPDF é¡µé¢å¯¹è±¡
        dpi: æ¸²æŸ“ DPI

    Returns:
        PIL Image å¯¹è±¡
    """
    pix = page.get_pixmap(dpi=dpi)
    img_data = pix.tobytes("png")
    return Image.open(fitz.io.BytesIO(img_data))


class AIMarkdownTranslator:
    """AIæ–‡æ¡£ç¿»è¯‘å™¨ - Markdownæ¨¡å¼ï¼ˆVLè¯†åˆ« + MTç¿»è¯‘ï¼‰"""

    def __init__(
        self,
        vl_model: str = "plus",
        mt_model: str = "qwen-mt-plus",
        region: str = "beijing",
        dpi: int = 300,
        api_key: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–ç¿»è¯‘å™¨

        Args:
            vl_model: VLæ¨¡å‹é€‰æ‹© (flash/plus/ocr)
            mt_model: ç¿»è¯‘æ¨¡å‹åç§°
            region: åŒºåŸŸ (beijing/singapore)
            dpi: PDFæ¸²æŸ“DPI
            api_key: APIå¯†é’¥
        """
        self.dpi = dpi

        # VLæ¨¡å‹ç”¨äºæ–‡æ¡£è¯†åˆ«
        self.ocr = QwenVLOCR(model=vl_model, region=region, api_key=api_key)

        # ä¸“ç”¨ç¿»è¯‘æ¨¡å‹
        self.translator = QwenMTTranslator(api_key=api_key, region=region, model=mt_model)

    def translate(
        self,
        file_path: Path,
        target_lang: str,
        source_lang: str = "auto",
        pages: Optional[List[int]] = None,
        domain: Optional[str] = None,
        glossary_path: Optional[Path] = None,
        preserve_original: bool = False,
        progress_callback: Optional[callable] = None,
    ) -> str:
        """
        ç¿»è¯‘PDFæ–‡æ¡£ä¸ºMarkdown

        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            source_lang: æºè¯­è¨€ä»£ç ï¼Œé»˜è®¤ auto è‡ªåŠ¨æ£€æµ‹
            pages: é¡µé¢åˆ—è¡¨ï¼ˆ0-basedç´¢å¼•ï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
            domain: é¢†åŸŸæç¤ºï¼ˆè‹±æ–‡æè¿°ï¼‰
            glossary_path: æœ¯è¯­è¡¨CSVæ–‡ä»¶è·¯å¾„
            preserve_original: æ˜¯å¦åœ¨è¾“å‡ºä¸­ä¿ç•™åŸæ–‡
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç­¾åä¸º (current, total, description, advance)

        Returns:
            Markdownæ ¼å¼çš„ç¿»è¯‘ç»“æœ
        """
        # éªŒè¯è¯­è¨€å¯¹
        is_valid, error = self.translator.validate_language_pair(source_lang, target_lang)
        if not is_valid:
            raise ValueError(error)

        # æ‰“å¼€PDF
        doc = fitz.open(file_path)
        if pages is None:
            pages = list(range(doc.page_count))

        total = len(pages)
        results = []

        # åŠ è½½æœ¯è¯­è¡¨
        terminologies = self._load_glossary(glossary_path) if glossary_path else None

        # æ„å»ºé¢†åŸŸæç¤º
        domain_prompt = self._build_domain_prompt(domain, terminologies)

        for idx, page_num in enumerate(pages):
            # é˜¶æ®µ1: VLæ¨¡å‹æå–åŸæ–‡
            if progress_callback:
                progress_callback(idx, total, f"[warning]æå–[/][black]ç¬¬[/] [bold_text]{page_num + 1}[/] [black]é¡µåŸæ–‡[/]", False)

            image = pdf_page_to_image(doc[page_num], dpi=self.dpi)
            original_text = self.ocr.ocr_image(image, prompt=OCR_EXTRACT_PROMPT)

            if not original_text or not original_text.strip():
                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(idx + 1, total, f"[success]å®Œæˆç¬¬ {page_num + 1} é¡µç©ºç™½[/]", True)
                continue

            # é˜¶æ®µ2: ä¸“ç”¨æ¨¡å‹ç¿»è¯‘
            if progress_callback:
                progress_callback(idx, total, f"[info]ç¿»è¯‘[/][black]ç¬¬[/] [bold_text]{page_num + 1}[/] [black]é¡µ[/]", False)

            translated_text = self.translator.translate(
                text=original_text,
                target_lang=target_lang,
                source_lang=source_lang,
                domain=domain_prompt,
                terminologies=terminologies,
            )

            results.append({
                "page": page_num + 1,
                "original": original_text if preserve_original else None,
                "translated": translated_text,
            })

            # æ›´æ–°è¿›åº¦
            if progress_callback:
                progress_callback(idx + 1, total, f"[success]å®Œæˆç¬¬ {page_num + 1} é¡µ[/]", True)

        doc.close()

        return self._format_output(
            file_path, target_lang, source_lang, results, preserve_original
        )

    def _build_domain_prompt(
        self,
        domain: Optional[str],
        terminologies: Optional[List[Dict[str, str]]],
    ) -> str:
        """æ„å»ºé¢†åŸŸæç¤º

        Args:
            domain: é¢†åŸŸæç¤º
            terminologies: æœ¯è¯­è¡¨

        Returns:
            åˆå¹¶åçš„é¢†åŸŸæç¤º
        """
        parts = []

        if domain:
            parts.append(domain)

        # æœ¯è¯­è¡¨æç¤ºä¼šåœ¨ translate æ–¹æ³•ä¸­é€šè¿‡ system message ä¼ é€’
        # è¿™é‡Œä¸éœ€è¦é‡å¤æ·»åŠ 

        return " ".join(parts) if parts else None

    def _load_glossary(self, path: Path) -> List[Dict[str, str]]:
        """åŠ è½½æœ¯è¯­è¡¨CSV

        Args:
            path: CSVæ–‡ä»¶è·¯å¾„

        Returns:
            æœ¯è¯­è¡¨åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"src": "...", "tgt": "..."}, ...]
        """
        glossary = []
        try:
            with open(path, newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    src = row.get("src", "").strip()
                    tgt = row.get("tgt", "").strip()
                    if src and tgt:
                        glossary.append({"src": src, "tgt": tgt})
        except Exception as e:
            raise ValueError(f"åŠ è½½æœ¯è¯­è¡¨å¤±è´¥: {e}")

        return glossary

    def _format_output(
        self,
        file_path: Path,
        target_lang: str,
        source_lang: str,
        results: List[Dict],
        preserve_original: bool,
    ) -> str:
        """æ ¼å¼åŒ–Markdownè¾“å‡º

        Args:
            file_path: æºæ–‡ä»¶è·¯å¾„
            target_lang: ç›®æ ‡è¯­è¨€
            source_lang: æºè¯­è¨€
            results: ç¿»è¯‘ç»“æœåˆ—è¡¨
            preserve_original: æ˜¯å¦ä¿ç•™åŸæ–‡

        Returns:
            æ ¼å¼åŒ–åçš„Markdownæ–‡æœ¬
        """
        lines = [
            "# æ–‡æ¡£ç¿»è¯‘\n",
            f"**æºæ–‡ä»¶**: {file_path.name}  ",
            f"**ç¿»è¯‘æ–¹å‘**: {source_lang} â†’ {target_lang}  ",
            f"**ç¿»è¯‘æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  ",
            f"**ç¿»è¯‘æ¨¡å¼**: Markdown (VL + MT)\n",
            "---\n",
        ]

        for item in results:
            lines.append(f"## ç¬¬ {item['page']} é¡µ\n")

            # ä¿ç•™åŸæ–‡ï¼ˆæŠ˜å æ˜¾ç¤ºï¼‰
            if preserve_original and item.get("original"):
                lines.append("<details>")
                lines.append("<summary>ğŸ“„ æŸ¥çœ‹åŸæ–‡</summary>\n")
                lines.append(item["original"])
                lines.append("\n</details>\n")

            # ç¿»è¯‘ç»“æœ
            lines.append(item["translated"])
            lines.append("\n---\n")

        return "\n".join(lines)
